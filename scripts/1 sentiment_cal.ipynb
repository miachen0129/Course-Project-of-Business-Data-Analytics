{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5384101-6a39-46ef-b008-24cf227f8c3d",
   "metadata": {},
   "source": [
    "# 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111651c6-d1ab-4543-99e7-1dad5f104298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T02:48:34.341487Z",
     "start_time": "2025-05-30T02:48:34.331801Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"../data/raw.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c2f71-5ee5-481e-8246-71f7e3275c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocessing():\n",
    "    for i in range(1,4):\n",
    "        # X1,X2,X3分别讨论\n",
    "        total_col = 'X%s'%i # 生成总分列名\n",
    "        cols = ['X%s%s'%(i,j) for j in range(1,5)] # 生成小分列名\n",
    "        for j in range(4):\n",
    "            # 找出满足Xij>25的数据 of 空值\n",
    "            dfd = df[df[cols[j]] > 25 | df[cols[j]].isnull()]\n",
    "            if dfd.empty:\n",
    "                continue\n",
    "            # 按照总分反推小分\n",
    "            dfd.loc[:, cols[j]] = dfd.loc[:,total_col] - dfd.loc[:,cols[(j+1)%4]] - dfd.loc[:,cols[(j+2)%4]] - dfd.loc[:,cols[(j+3)%4]]\n",
    "            # 更新修改的数据行\n",
    "            df.update(dfd)\n",
    "        # 处理总分与小分不一致的情况\n",
    "        df.loc[:,total_col] = dfd.loc[:,cols[0]:cols[-1]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc64aa-e57c-4b61-a412-b11db40fcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算总分\n",
    "def totalCal():\n",
    "    df['Avg1'] = df[['X11','X21','X31']].mean(axis=1)\n",
    "    df['Avg2'] = df[['X12','X22','X32']].mean(axis=1)\n",
    "    df['Avg3'] = df[['X13','X23','X33']].mean(axis=1)\n",
    "    df['Avg4'] = df[['X14','X24','X34']].mean(axis=1)\n",
    "    df['Avg'] = df[['X1','X2','X3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b1e37-557a-4fdd-9389-44c5592ba418",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing()\n",
    "totalCal()\n",
    "\n",
    "df.to_excel('../data/preprocessed/preprocessed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f0cd8-a606-4761-ac36-9beda04399cd",
   "metadata": {},
   "source": [
    "# 2. Comment Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40d14a-4b22-4aa1-89ea-56db6a65735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jieba import lcut\n",
    "from jieba import cut\n",
    "import jieba.posseg\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976be90",
   "metadata": {},
   "source": [
    "## 2.1 Tools Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985a836-927d-495f-add1-5af9366df5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "def loadStopwords():\n",
    "    stopwords1 = pd.read_csv('/Users/antonchekhov/Desktop/BA_nlp/stopwords/中文停用词库.txt', encoding=\"gbk\",names=[\"stopword\"])[\"stopword\"].tolist()\n",
    "    stopwords2 = pd.read_csv('/Users/antonchekhov/Desktop/BA_nlp/stopwords/百度停用词列表.txt', encoding=\"gbk\",names=[\"stopword\"])[\"stopword\"].tolist()\n",
    "    stopwords3 = pd.read_csv('/Users/antonchekhov/Desktop/BA_nlp/stopwords/四川大学机器智能实验室停用词库.txt',encoding=\"gbk\",names=[\"stopword\"])[\"stopword\"].tolist()\n",
    "    stopwords = stopwords3 + stopwords2 + stopwords1\n",
    "    stopwords.extend(list(';: .)(（）-——①②③④⑤⑥⑦⑧⑨'))\n",
    "    return stopwords\n",
    "# stopwords = loadStopwords()\n",
    "\n",
    "\n",
    "# tokinize\n",
    "def tokenize(sent, func):\n",
    "    \"\"\"\n",
    "    去停用词+分词操作\n",
    "    :param sent: 短句文本\n",
    "    :param func: 分词操作函数\n",
    "    :param stopwords: 停用词表\n",
    "    :return: 词语列表\n",
    "    \"\"\"\n",
    "    # 将句子分割成词语列表\n",
    "    words = func(sent)\n",
    "    target_ls = []\n",
    "    for word in words:\n",
    "        # 去除停用词\n",
    "        # 结果中可能包含重复词汇\n",
    "        if word not in stopwords:\n",
    "            target_ls.append(word)\n",
    "    return target_ls\n",
    "\n",
    "# tokenize according to pos\n",
    "def cut_words_with_pos(text):\n",
    "    seg = jieba.posseg.cut(text)\n",
    "    res = []\n",
    "    for i in seg:\n",
    "        if i.flag in [\"a\",\"ad\", \"n\", \"an\", \"vn\", \"nz\", \"nt\", \"nr\"]:\n",
    "            res.append(i.word)\n",
    "    return list(res)\n",
    "\n",
    "# sentence segmentation\n",
    "def sentCut(sents):\n",
    "    \"\"\"\n",
    "    将评论分为短句（不分词）\n",
    "    :param sents: 整段评论文字\n",
    "    :return: 短句列表\n",
    "    \"\"\"\n",
    "    if type(sents) != str:\n",
    "        return []\n",
    "    sents = sents.strip() # 去除前后空格\n",
    "    sent_ls = re.split(r'，|。|；|\\0',sents) # 按句号、分号、逗号进行划分\n",
    "    sent_ls = [s for s in sent_ls if len(s)>0] # 去除空字符\n",
    "    return sent_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb6540-3dc6-4930-9d99-f16b6276fe29",
   "metadata": {},
   "source": [
    "## 2.2 Construct Feature Word Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa234259-1cf1-4d15-a7cd-bf162b5ad860",
   "metadata": {},
   "source": [
    "1. 用于模型（tf-idf, word2vec）训练的数据：所有评论分词+去停用词；全部整合在一起 （total_text list）\n",
    "\n",
    "2. 用于构建词典的分专业类别评论：grouped; 按专业整合再在一起 (subject_map dict)\n",
    "\n",
    "3. 用于每篇论文情感计算和打分的评论 (df[\"RList\"])\n",
    "\n",
    "4. 评价标准处理——分词，提取关键词 (target_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbf82b-f761-46f3-a678-32803e9fa947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预处理后的数据\n",
    "df = pd.read_excel('../data/preprocessed/preprocessed.xlsx')\n",
    "\n",
    "# 保留评论列R1,R2,R3\n",
    "reviews_df = df[['Tag','R1','R2','R3']]\n",
    "for col in ['R1','R2','R2']:\n",
    "    reviews_df[col].map(str)\n",
    "# 删除空值，此为全部删除\n",
    "# 若指定超过两个空值就删除：df.dropna(axis = 0, thresh = 2)\n",
    "reviews_df = reviews_df.dropna()\n",
    "\n",
    "# 将评论数据处理为短句列表\n",
    "# [\" \",sent2,...]\n",
    "# 存入review_df中的R_ls中\n",
    "R_ls = []\n",
    "for row in reviews_df.itertuples():\n",
    "    # 遍历每一行\n",
    "    r_ls = [sentCut(row.R1), sentCut(row.R2), sentCut(row.R3)]\n",
    "    R_ls.append(r_ls)\n",
    "reviews_df.loc[:,\"Rlist\"] = R_ls\n",
    "\n",
    "# 按照tag分组\n",
    "grouped_reviews = reviews_df.groupby(df['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b55185-7c8b-48f1-a437-c156b6117f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,group in grouped_reviews:\n",
    "    print('---------',name,'----------')\n",
    "    print(group.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a7adb-0c6a-457f-9dc6-02caf9ad7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62677b79-52cc-41c4-b22c-a96cd560128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "text_df = reviews_df[reviews_df['Tag']==8]\n",
    "R_ls = \"\"\n",
    "for row in text_df.itertuples():\n",
    "    # 遍历每一行\n",
    "    for col in range(2,5):\n",
    "        if type(row[col]) != float:\n",
    "            R_ls += row[col]\n",
    "    \n",
    "words = lcut(R_ls)\n",
    "words = [word for word in words if word not in stopwords]\n",
    "\n",
    "from collections import Counter\n",
    "result = Counter(words).most_common(20) #取最多的50组\n",
    "#print(result)\n",
    "\n",
    "# 绘制词云图\n",
    "from wordcloud import WordCloud #导入相关库\n",
    "import wordcloud\n",
    "content = ' '.join(words) #把列表转换为字符串\n",
    "font_path=\"/System/Library/fonts/PingFang.ttc\"\n",
    "wc = WordCloud(font_path = font_path,\n",
    "               color_func=wordcloud.get_single_color_func(\"black\"),\n",
    "               background_color='white',#背景颜色（这里为白色）\n",
    "                width=1000,#宽度\n",
    "                height=600,#高度\n",
    "                 ).generate(content) #绘制词云图\n",
    "wc.to_file('WordCloud_8.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac50fd-02e9-4fa7-9a38-115a79a3d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05f764-2e1f-4f60-bdf8-5aa63ac87562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict: 按专业存储经过分词的文本数据\n",
    "# key: 专业编号\n",
    "# value: 字符串列表，列表每个元素代表一个短句，每个短句形式为“word1 word2 word3”\n",
    "subject_map = {}\n",
    "total_text = []\n",
    "for name, group in grouped_reviews:\n",
    "    # 以列表形式储存字符，每个元素为一段评语\n",
    "    sent_ls = []\n",
    "    for row in group.itertuples():\n",
    "        rlist = row.Rlist\n",
    "        sent = rlist[0] + rlist[1] + rlist[2]\n",
    "        sent = [\" \".join(cut_words_with_pos(r)) for r in sent]\n",
    "        sent_ls += sent\n",
    "    subject_map[name] = sent_ls\n",
    "    total_text += (sent_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6bb09-0c2b-4736-ab3f-8a65e84bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评价要素\n",
    "targets = [\"选题与综述：研究的理论意义，实用；对本学科及相关学科领域国内外发展状况和 学术动态的了解程度\",\n",
    "            \"创新性及论文价值：论文提出的新见解、新方法所具有的价值.论文成果对科技进步、经济建设、国家安全等方面产生的影响或作用\",\n",
    "            \"科研能力与基础知识：论文体现的理论基础的扎实程度；本学科及相关学科领域专门知识的系统性；分析问题、解决问题的能力；研究方法的科学性，是否采用先进技术、设备、信息等进行论文研究工作。\",\n",
    "            \"论文规范性:引文的规范性，学风的严谨性；论文语言表达的准确性、逻辑的严密性、书写格式及图表的规范性\" ]\n",
    "\n",
    "\n",
    "# 评价指标部分分词分词\n",
    "# map: 第i指标 -> keywords\n",
    "target_dict = {}\n",
    "for i, target in enumerate(targets):\n",
    "    words = tokenize(target, lcut)\n",
    "    word_set = []\n",
    "    # 去除重复词语\n",
    "    for word in words:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    "    target_dict[i] = word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e52c2-5385-4df6-b1c2-06ac33f373a9",
   "metadata": {},
   "source": [
    "### 2.2.1 Latent Dirichlet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ab44c-0e26-4140-90ed-82bc177e3bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59af6bd-f236-46a3-9eec-f9ef96d405e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda各专业主题\n",
    "def lda(text):\n",
    "    dictionary = corpora.Dictionary(text)  # 构建词典\n",
    "    corpus = [dictionary.doc2bow(t) for t in text]  #表示为第几个单词出现了几次\n",
    "    num_topics=4\n",
    "    ldamodel = LdaModel(corpus, num_topics=40, id2word = dictionary, passes=30,random_state = 1)   #分为4个主题\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=10))  #每个主题输出10个单词\n",
    "\n",
    "    \n",
    "for key,value in subject_map.items():\n",
    "    text = [sent.split() for sent in value]\n",
    "    print(\"----------Tag = %s----------\"%key)\n",
    "    lda(text)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce296a0-0660-4a18-85b3-1e2f13da4021",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2.2 modified K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c797b94-a05c-4a7a-8f60-3c88270a9d99",
   "metadata": {},
   "source": [
    "#### K-Means base on Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15b7db-c12c-4a1c-860c-48976b4597e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e85d49-4a49-49cb-b660-b5a9a0f6c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(cut_word_list, model):\n",
    "    features = []\n",
    "    for tokens in cut_word_list:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "def KmeansByWord2vec(tag,maxN):\n",
    "    \"\"\"\n",
    "    :param tag: tag\n",
    "    :param maxN: 最大输出词数\n",
    "    :return: 各学科补充词列表\n",
    "    \"\"\"\n",
    "    docs = subject_map[tag]\n",
    "    docs = [doc.split() for doc in docs]\n",
    "    model = Word2Vec(sentences=docs, vector_size=10, workers=1, seed=SEED)\n",
    "    X = vectorize(docs, model=model)\n",
    "    keys = model.wv.key_to_index\n",
    "    kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    result_ls = [[],[],[],[]]\n",
    "    distance_ls = [[],[],[],[]]\n",
    "    for key,value in keys.items():\n",
    "        index = labels[value]\n",
    "        result_ls[index].append(key)\n",
    "        dist = np.dot((centroids[index]-value).T,(centroids[index]-value))\n",
    "        distance_ls[index].append(dist)\n",
    "    sorted_result = sorted(result_ls, key=lambda x: distance_ls[result_ls.index(x)])\n",
    "    for i in range(len(sorted_result)):\n",
    "        res = sorted_result[i]\n",
    "        if len(res) > maxN: \n",
    "            sorted_result[i] = res[:maxN]\n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d18e65-3e2a-4565-b9bd-9c08a5147f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以Tag=1,maxN=20为例\n",
    "KmeansByWord2vec(1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c01a3c-6d0e-4f28-8e3b-4053b6d76eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "write = pd.ExcelWriter(\"/Users/antonchekhov/Desktop/Keywords.xlsx\")\n",
    "maxN = 20\n",
    "for key in subject_map.keys():\n",
    "    print(\"----------Tag = %s----------\"%key)\n",
    "    KWs = KmeansByWord2vec(key,maxN)\n",
    "    # KWs = pd.DataFrame(KWs)\n",
    "    print(KWs)\n",
    "    # KWs.to_excel(write,'KM%s'% key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2147d-7f2f-43b3-aaae-386012615f23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Sentiment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640d602-fdd4-43be-91bb-9cb537f5f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc63e2-cacd-4268-8269-53e11169da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '选题具有理论意义和现实意义，论文语言流畅，总体架构基本合理'\n",
    "s2 = '论文写作粗糙，表格图表不严谨。'\n",
    "\n",
    "for s in [s1,s2]:\n",
    "    print('The sentence: ',s)\n",
    "    print('The sentiment score of the sentence: ', SnowNLP(s).sentiments)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a687a-24a4-49b0-8bc8-b7dc3007e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentsCal(text, target_dict):\n",
    "    \"\"\"\n",
    "    计算短句的情感值\n",
    "    :param text: 短句文本\n",
    "    :param target_dict: 指标词典\n",
    "    :return: 情感值列表[,,]\n",
    "    \"\"\"\n",
    "    scores = [0,0,0,0]\n",
    "    count = [0,0,0,0]\n",
    "    sentiment = SnowNLP(text).sentiments\n",
    "    words = tokenize(text, lcut)\n",
    "    '''\n",
    "    for key, value in enumerate(target_dict):\n",
    "        count[key] = len(value)\n",
    "        for kw in value:\n",
    "            if kw in words:\n",
    "                scores[key] += 25\n",
    "            else:\n",
    "                scores[key] += 22.5\n",
    "    scores = [ scores[i]/count[i]*sentiment for i in range(4) ]\n",
    "    '''\n",
    "    for key,value in enumerate(target_dict):\n",
    "        for kw in value:\n",
    "            if kw in words:\n",
    "                scores[key] += sentiment\n",
    "                count[key] += 1\n",
    "    for i,cnt in enumerate(count):\n",
    "        if cnt == 0:\n",
    "            scores[i] = 1\n",
    "        else:\n",
    "            scores[i] = scores[i] / count[i]\n",
    "\n",
    "    return scores\n",
    "\n",
    "def ReviewSentiments(reviews,target_dict):\n",
    "    \"\"\"\n",
    "    计算句子列表的情感值\n",
    "    :param reviews: 短句列表\n",
    "    :return: 情感值列表np.array([,,])\n",
    "    \"\"\"\n",
    "    num = len(reviews)\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    scores = [ sentimentsCal(review,target_dict) for review in reviews]\n",
    "    scores = np.array(scores)\n",
    "    mean =np.sum(scores,axis=0)/num\n",
    "    return mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a48a61-2b02-415b-b239-c4cfaf638350",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../results/feature_dictionary/mergeDict.xlsx\"\n",
    "cols = [\"选题与综述\",\"创新与论文价值\",\"科研能力与基础知识\",\"论文规范性\"]\n",
    "new_reviews_df = reviews_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a885f-ec7a-41bc-bf19-deb34e330170",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[],[],[]]\n",
    "\n",
    "for row in reviews_df.itertuples():\n",
    "    review_ls = row.Rlist\n",
    "    tag = row.Tag\n",
    "    keyword_df = pd.read_excel(path,str(tag))\n",
    "    keyword_dict = [list(keyword_df[col]) for col in cols]\n",
    "    for i,reviews in enumerate(review_ls):\n",
    "        score = ReviewSentiments(reviews, keyword_dict)\n",
    "        scores[i].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfe549-fe99-455a-8d9e-db499ee19cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    score_ls = scores[i-1]\n",
    "    for j in range(1,5):\n",
    "        sls = []\n",
    "        for score in score_ls:\n",
    "            if type(score) == int:\n",
    "                sls.append(-1)\n",
    "            else:\n",
    "                sls.append(score[j-1])\n",
    "        new_reviews_df[\"S%s%s\"%(i,j)] = pd.Series(sls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186823c-2bba-494e-a4be-4bbd3ce656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_reviews_df.head(10)\n",
    "# new_reviews_df.to_excel(\"result.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
